#!/bin/bash
# download_wikipedia_data

# Function definition
run() {
  COMMAND=$1
  echo "$COMMAND"
  eval $COMMAND
}

# 0. Install 
run "pip install wikiextractor"

# 1. Download
LINK2DATA=https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2
run "nohup wget $LINK2DATA &"
run "tail -n 3 nohup.out"

# 2. Wait
# Get the PID of the wget process
wget_pid=$(pgrep wget)

# Wait until the wget process is no longer running
while kill -0 $wget_pid 2> /dev/null; do
  sleep 1
done

# The wget process has finished downloading the file
echo "wget has finished downloading the file."
run "tail -n 3 nohup.out"

# 3. Extract raw data
run "nohup python -m wikiextractor.WikiExtractor enwiki-latest-pages-articles.xml.bz2 --json &"
run "tree -d text/"  # to see the directory structure
run "nohup find text -name 'wiki_*' -exec cat {} \; > train_data.jsonl &"
