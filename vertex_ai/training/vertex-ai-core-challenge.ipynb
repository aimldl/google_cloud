{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:custom,training,online_prediction"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this notebook, you will learn how to create a custom-trained model from a Python script in a Docker container using the Vertex AI SDK for Python, and then get a prediction from the deployed model by sending data.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services and resources:\n",
    "\n",
    "- BigQuery\n",
    "- Cloud Storage\n",
    "- Vertex AI managed Datasets\n",
    "- Vertex AI Training\n",
    "- Vertex AI Endpoints\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Create a Vertex AI custom `TrainingPipeline` for training a model.\n",
    "- Train a TensorFlow model.\n",
    "- Deploy the `Model` resource to a serving `Endpoint` resource.\n",
    "- Make a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:custom,cifar10,icn"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used for this tutorial is the penguins dataset from [BigQuery public datasets](https://cloud.google.com/bigquery/public-data). For this tutorial, you use only the fields `culmen_length_mm`, `culmen_depth_mm`, `flipper_length_mm`, `body_mass_g` from the dataset to predict the penguins species (`species`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the latest version of Cloud Storage, Bigquery and Vertex AI SDKs for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1fd00fa70a2a"
   },
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "! pip3 install --upgrade google-cloud-aiplatform \\\n",
    "                        google-cloud-storage \\\n",
    "                        'google-cloud-bigquery[pandas]'\n",
    "\n",
    "#automatically restarts kernel\n",
    "import IPython \n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a47846030fef"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "#### Set your project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3c8049930470",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT=!(gcloud config get-value project)\n",
    "PROJECT_ID=PROJECT[0]\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a54f9d7c1876",
    "tags": []
   },
   "source": [
    "#### Region\n",
    "\n",
    "Set the `REGION` variable as per the lab instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3aaadaaf9b30",
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = \"us-east1\"  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bucket",
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"qwiklabs-gcp-02-f695bba24cbb-cepf\" # update it from the lab instructions\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "750d53e37094"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "c9d3ac73dfbc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize the Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7c163842eabd"
   },
   "source": [
    "### Initialize BigQuery Client\n",
    "\n",
    "Initialize the BigQuery Python client for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fad2ba1ad7c3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Set up BigQuery client\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a2c41bc91a6"
   },
   "source": [
    "## Task 4. Create a Vertex AI Tabular Dataset from the BigQuery dataset\n",
    "\n",
    "### Preprocess data and split data\n",
    "- Convert categorical features to numeric\n",
    "- Split train and test data in the fration 80-20 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "e3a2449cfcf1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/cloud/bigquery/table.py:2309: UserWarning: Unable to represent RANGE schema as struct using pandas ArrowDtype. Using `object` instead. To use ArrowDtype, use pandas >= 1.5 and pyarrow >= 10.0.1.\n",
      "  warnings.warn(_RANGE_PYARROW_WARNING)\n",
      "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/cloud/bigquery/table.py:2323: UserWarning: Unable to represent RANGE schema as struct using pandas ArrowDtype. Using `object` instead. To use ArrowDtype, use pandas >= 1.5 and pyarrow >= 10.0.1.\n",
      "  warnings.warn(_RANGE_PYARROW_WARNING)\n",
      "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/cloud/bigquery/table.py:2337: UserWarning: Unable to represent RANGE schema as struct using pandas ArrowDtype. Using `object` instead. To use ArrowDtype, use pandas >= 1.5 and pyarrow >= 10.0.1.\n",
      "  warnings.warn(_RANGE_PYARROW_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "LABEL_COLUMN = \"species\"\n",
    "\n",
    "# Define the BigQuery source dataset\n",
    "BQ_SOURCE = \"bigquery-public-data.ml_datasets.penguins\"\n",
    "\n",
    "# Define NA values\n",
    "NA_VALUES = [\"NA\", \".\"]\n",
    "\n",
    "# Download a table\n",
    "table = bq_client.get_table(BQ_SOURCE)\n",
    "df = bq_client.list_rows(table).to_dataframe()\n",
    "\n",
    "# Drop unusable rows\n",
    "df = df.replace(to_replace=NA_VALUES, value=np.NaN).dropna()\n",
    "\n",
    "# Convert categorical columns to numeric\n",
    "df[\"island\"], _ = pd.factorize(df[\"island\"])\n",
    "df[\"species\"], _ = pd.factorize(df[\"species\"])\n",
    "df[\"sex\"], _ = pd.factorize(df[\"sex\"])\n",
    "\n",
    "# Split into a training and holdout dataset\n",
    "df_train = df.sample(frac=0.8, random_state=100)\n",
    "df_holdout = df[~df.index.isin(df_train.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a39df4692a70"
   },
   "source": [
    "### Create a Vertex AI Tabular Dataset\n",
    "\n",
    "Create a Vertex AI tabular dataset resource from BigQuery training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7fa452ee5c75",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset qwiklabs-gcp-02-f695bba24cbb.cepf_penguins_dataset\n"
     ]
    }
   ],
   "source": [
    "# Create BigQuery dataset\n",
    "BQ_DATASET=\"cepf_penguins_dataset\"\n",
    "bq_dataset_id = f\"{PROJECT_ID}.{BQ_DATASET}\"\n",
    "bq_dataset = bigquery.Dataset(bq_dataset_id)\n",
    "dataset = bq_client.create_dataset(bq_dataset, exists_ok=True)\n",
    "# Add dataset = next time.\n",
    "# dataset = bq_client.create_dataset(bq_dataset, exists_ok=True)\n",
    "print(\"Created dataset {}.{}\".format(bq_client.project, dataset.dataset_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Vertex AI tabular dataset from BigQuery training data\n",
    "#df_source=df_train\n",
    "#staging_path=table name provided in lab instructions\n",
    "#display_name=as provided in the lab instructions\n",
    "\n",
    "[ TODO - Insert your code ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create and populate the BigQuery table FIRST\n",
    "TABLE_NAME = \"cepf_penguins_table\"\n",
    "table_id = f\"{PROJECT_ID}.{BQ_DATASET}.{TABLE_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'species': int64\n",
      "Column 'island': int64\n",
      "Column 'culmen_length_mm': float64\n",
      "Column 'culmen_depth_mm': float64\n",
      "Column 'flipper_length_mm': float64\n",
      "Column 'body_mass_g': float64\n",
      "Column 'sex': int64\n"
     ]
    }
   ],
   "source": [
    "for col in df_train.columns:\n",
    "    print(f\"Column '{col}': {df_train[col].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# **Load df_train into BigQuery**\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    # Specify the schema of your df_train DataFrame if necessary\n",
    "    # \t\t\tculmen_length_mm\tculmen_depth_mm\tflipper_length_mm\tbody_mass_g\tsex\n",
    "    schema=[\n",
    "        bigquery.SchemaField(\"species\", \"int64\"),\n",
    "        bigquery.SchemaField(\"island\", \"int64\"),\n",
    "        bigquery.SchemaField(\"culmen_length_mm\", \"float64\"),\n",
    "        bigquery.SchemaField(\"culmen_depth_mm\", \"float64\"),\n",
    "        bigquery.SchemaField(\"flipper_length_mm\", \"float64\"),\n",
    "        bigquery.SchemaField(\"body_mass_g\", \"float64\"),\n",
    "        bigquery.SchemaField(\"sex\", \"int64\"),\n",
    "    ],\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # Overwrite the table if it exists\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=qwiklabs-gcp-02-f695bba24cbb, location=US, id=3b722711-0341-4f26-912f-50b499966055>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_source=df_train\n",
    "job = bq_client.load_table_from_dataframe(\n",
    "    df_source, table_id, job_config=job_config\n",
    ")  # Make an API request.\n",
    "job.result()  # Wait for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TabularDataset\n",
      "Create TabularDataset backing LRO: projects/101459138402/locations/us-east1/datasets/5743408938850713600/operations/7912068768484818944\n",
      "TabularDataset created. Resource name: projects/101459138402/locations/us-east1/datasets/5743408938850713600\n",
      "To use this TabularDataset in another session:\n",
      "ds = aiplatform.TabularDataset('projects/101459138402/locations/us-east1/datasets/5743408938850713600')\n"
     ]
    }
   ],
   "source": [
    "# *** Now create the Vertex AI TabularDataset ***\n",
    "staging_path=f\"bq://{bq_dataset_id}.cepf_penguins_table\"\n",
    "display_name=\"cepf_penguins\"\n",
    "\n",
    "#[ TODO - Insert your code ]\n",
    "dataset = aiplatform.TabularDataset.create(\n",
    "    display_name=display_name,\n",
    "    bq_source=f\"{staging_path}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_model"
   },
   "source": [
    "## Task 5. Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1npiDcUtlugw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the command args for the training script\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 10\n",
    "#LABEL_COLUMN = \"species\"  is defined above\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--label_column=\" + LABEL_COLUMN,\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--batch_size=\" + str(BATCH_SIZE),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taskpy_contents"
   },
   "source": [
    "#### Training script\n",
    "\n",
    "Complete the contents of the training script, `task.py`. You need to write code in the **[ TODO - Insert your code ]** section by training the model with epochs and batch size according and saves the trained model artifact to Cloud Storage directory `aiplatform-custom-training` in the created Cloud Storage Bucket location using `os.environ['AIP_MODEL_DIR']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "72rUqXNFlugx",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile task.py\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# Read environmental variables\n",
    "training_data_uri = os.getenv(\"AIP_TRAINING_DATA_URI\")\n",
    "validation_data_uri = os.getenv(\"AIP_VALIDATION_DATA_URI\")\n",
    "test_data_uri = os.getenv(\"AIP_TEST_DATA_URI\")\n",
    "\n",
    "# Added by T Kim\n",
    "print(\"=========================================\")\n",
    "print(f\"training_data_uri={training_data_uri}\")\n",
    "print(f\"validation_data_uri={validation_data_uri}\")\n",
    "print(f\"test_data_uri={test_data_uri}\")\n",
    "\n",
    "aip_model_dir = os.getenv(\"AIP_MODEL_DIR\")\n",
    "print(f\"aip_model_dir={aip_model_dir}\")\n",
    "print(\"=========================================\")\n",
    "# Added by T Kim\n",
    "\n",
    "# Read args\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--label_column', required=True, type=str)\n",
    "parser.add_argument('--epochs', default=10, type=int)\n",
    "parser.add_argument('--batch_size', default=10, type=int)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Set up training variables\n",
    "LABEL_COLUMN = args.label_column\n",
    "\n",
    "# See https://cloud.google.com/vertex-ai/docs/workbench/managed/executor#explicit-project-selection for issues regarding permissions.\n",
    "PROJECT_NUMBER = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "bq_client = bigquery.Client(project=PROJECT_NUMBER)\n",
    "\n",
    "\n",
    "# Download a table\n",
    "def download_table(bq_table_uri: str):\n",
    "    # Remove bq:// prefix if present\n",
    "    prefix = \"bq://\"\n",
    "    if bq_table_uri.startswith(prefix):\n",
    "        bq_table_uri = bq_table_uri[len(prefix) :]\n",
    "        \n",
    "    # Download the BigQuery table as a dataframe\n",
    "    # This requires the \"BigQuery Read Session User\" role on the custom training service account.\n",
    "    table = bq_client.get_table(bq_table_uri)\n",
    "    return bq_client.list_rows(table).to_dataframe()\n",
    "\n",
    "# Download dataset splits\n",
    "df_train = download_table(training_data_uri)\n",
    "df_validation = download_table(validation_data_uri)\n",
    "df_test = download_table(test_data_uri)\n",
    "\n",
    "def convert_dataframe_to_dataset(\n",
    "    df_train: pd.DataFrame,\n",
    "    df_validation: pd.DataFrame,\n",
    "):\n",
    "    df_train_x, df_train_y = df_train, df_train.pop(LABEL_COLUMN)\n",
    "    df_validation_x, df_validation_y = df_validation, df_validation.pop(LABEL_COLUMN)\n",
    "\n",
    "    y_train = tf.convert_to_tensor(np.asarray(df_train_y).astype(\"float32\"))\n",
    "    y_validation = tf.convert_to_tensor(np.asarray(df_validation_y).astype(\"float32\"))\n",
    "\n",
    "    # Convert to numpy representation\n",
    "    x_train = tf.convert_to_tensor(np.asarray(df_train_x).astype(\"float32\"))\n",
    "    x_test = tf.convert_to_tensor(np.asarray(df_validation_x).astype(\"float32\"))\n",
    "\n",
    "    # Convert to one-hot representation\n",
    "    num_species = len(df_train_y.unique())\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_species)\n",
    "    y_validation = tf.keras.utils.to_categorical(y_validation, num_classes=num_species)\n",
    "\n",
    "    dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    dataset_validation = tf.data.Dataset.from_tensor_slices((x_test, y_validation))\n",
    "    return (dataset_train, dataset_validation)\n",
    "\n",
    "# Create datasets\n",
    "dataset_train, dataset_validation = convert_dataframe_to_dataset(df_train, df_validation)\n",
    "\n",
    "# Shuffle train set\n",
    "dataset_train = dataset_train.shuffle(len(df_train))\n",
    "\n",
    "def create_model(num_features):\n",
    "    # Create model\n",
    "    Dense = tf.keras.layers.Dense\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            Dense(\n",
    "                100,\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=\"uniform\",\n",
    "                input_dim=num_features,\n",
    "            ),\n",
    "            Dense(75, activation=tf.nn.relu),\n",
    "            Dense(50, activation=tf.nn.relu),            \n",
    "            Dense(25, activation=tf.nn.relu),\n",
    "            Dense(3, activation=tf.nn.softmax),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Compile Keras model\n",
    "    optimizer = tf.keras.optimizers.RMSprop(lr=0.001)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_model(num_features=dataset_train._flat_shapes[0].dims[0].value)\n",
    "\n",
    "# Set up datasets\n",
    "dataset_train = dataset_train.batch(args.batch_size)\n",
    "dataset_validation = dataset_validation.batch(args.batch_size)\n",
    "\n",
    "# Train the model\n",
    "model.fit(dataset_train, epochs=args.epochs, validation_data=dataset_validation)\n",
    "\n",
    "tf.saved_model.save(model, os.getenv(\"AIP_MODEL_DIR\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_job"
   },
   "source": [
    "### Executes script in Cloud Vertex AI Training\n",
    "\n",
    "Define your custom `TrainingPipeline` on Vertex AI.\n",
    "\n",
    "Use the `CustomTrainingJob` class to define the `TrainingPipeline`. The class takes the following parameters:\n",
    "\n",
    "- `display_name`: The user-defined name of this training pipeline.\n",
    "- `script_path`: The local path to the training script.\n",
    "- `container_uri`: The URI of the training container image.\n",
    "- `requirements`: The list of Python package dependencies of the script.\n",
    "- `model_serving_container_image_uri`: The URI of a container that can serve predictions for your model — either a pre-built container or a custom container.\n",
    "\n",
    "Use the `run` function to start training.\n",
    "\n",
    "The `run` function creates a training pipeline that trains and creates a `Model` object. After the training pipeline completes, the `run` function returns the `Model` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "mxIxvDdglugx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "JOB_NAME = \"cepf_custom_training_job\"\n",
    "MODEL_DISPLAY_NAME = \"cepf_penguins_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `CustomTrainingJob` class to define the `TrainingPipeline`.\n",
    "# container_uri=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\"\n",
    "# requirements=[\"google-cloud-bigquery[pandas]\", \"protobuf<3.20.0\"]\n",
    "# model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\",\n",
    "\n",
    "[ TODO - Insert your code ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    script_path=\"task.py\",\n",
    "    container_uri=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\",\n",
    "    requirements=[\"google-cloud-bigquery[pandas]\", \"protobuf<3.20.0\"],\n",
    "    model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\",\n",
    ")\n",
    "\n",
    "    # ... other parameters ...\n",
    "    #staging_bucket='gs://qwiklabs-gcp-02-f695bba24cbb-cepf'\n",
    "    #staging_bucket=BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "The error message \"AttributeError: 'str' object has no attribute 'name'\" indicates that you're passing a string (\"cepf_penguins\") to the dataset argument in job.run, while it expects a Dataset object.\n",
    "\n",
    "You've created a Vertex AI tabular dataset named \"cepf_penguins\". You need to retrieve that dataset as a Dataset object and pass that to job.run.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training script copied to:\n",
      "gs://qwiklabs-gcp-02-f695bba24cbb-cepf/aiplatform-2024-12-12-13:42:44.848-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
      "Training Output directory:\n",
      "gs://qwiklabs-gcp-02-f695bba24cbb-cepf/aiplatform-custom-training-2024-12-12-13:42:44.941 \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-east1/training/6731043746672017408?project=101459138402\n",
      "CustomTrainingJob projects/101459138402/locations/us-east1/trainingPipelines/6731043746672017408 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/101459138402/locations/us-east1/trainingPipelines/6731043746672017408 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/101459138402/locations/us-east1/trainingPipelines/6731043746672017408 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/101459138402/locations/us-east1/trainingPipelines/6731043746672017408 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/101459138402/locations/us-east1/trainingPipelines/6731043746672017408 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-east1/training/5172780683415781376?project=101459138402\n",
      "CustomTrainingJob projects/101459138402/locations/us-east1/trainingPipelines/6731043746672017408 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob run completed. Resource name: projects/101459138402/locations/us-east1/trainingPipelines/6731043746672017408\n",
      "Model available at projects/101459138402/locations/us-east1/models/1462407639546724352\n"
     ]
    }
   ],
   "source": [
    "# Run the training job\n",
    "training_fraction_split=0.8\n",
    "validation_fraction_split=0.1\n",
    "test_fraction_split=0.1\n",
    "\n",
    "model = job.run(\n",
    "    dataset=dataset,  #,\n",
    "    model_display_name=MODEL_DISPLAY_NAME,\n",
    "    bigquery_destination=f\"bq://{PROJECT_ID}\",\n",
    "    args=CMDARGS,\n",
    "    replica_count=1,  # Number of worker replicas\n",
    "    machine_type=\"c2-standard-8\",  #\"n1-standard-4\",\n",
    "    training_fraction_split=training_fraction_split,\n",
    "    validation_fraction_split=validation_fraction_split,\n",
    "    test_fraction_split=test_fraction_split,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deploy_model:dedicated"
   },
   "source": [
    "## Task 6. Deploy the model\n",
    "\n",
    "Before you use your model to make predictions, you must deploy it to an endpoint.\n",
    "\n",
    "1. Create an endpoint resource named `cepf_penguins_model_endpoint`.\n",
    "2. Deploy the model resource to the endpoint resource with the name `penguins_deployed`.\n",
    "\n",
    "------------\n",
    "1. Create an `Endpoint` resource for deploying the `Model` resource to.\n",
    "2. Deploy the `Model` resource to the `Endpoint` resource.\n",
    "\n",
    "https://cloud.google.com/vertex-ai/docs/general/deployment#python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/101459138402/locations/us-east1/endpoints/1190203744881475584/operations/3196799958627909632\n",
      "Endpoint created. Resource name: projects/101459138402/locations/us-east1/endpoints/1190203744881475584\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/101459138402/locations/us-east1/endpoints/1190203744881475584')\n"
     ]
    }
   ],
   "source": [
    "DEPLOYED_NAME = \"cepf_penguins_model_endpoint\"\n",
    "\n",
    "# Create an Endpoint resource\n",
    "endpoint = aiplatform.Endpoint.create(display_name=DEPLOYED_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model to Endpoint : projects/101459138402/locations/us-east1/endpoints/1190203744881475584\n",
      "Deploy Endpoint model backing LRO: projects/101459138402/locations/us-east1/endpoints/1190203744881475584/operations/7736428383017369600\n",
      "Endpoint model deployed. Resource name: projects/101459138402/locations/us-east1/endpoints/1190203744881475584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x7f93244e1ed0> \n",
       "resource name: projects/101459138402/locations/us-east1/endpoints/1190203744881475584"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEPLOYED_NAME = \"penguins_deployed\"\n",
    "\n",
    "# Deploy the model at model endpoint\n",
    "    # The explanation_metadata and explanation_parameters should only be\n",
    "    # provided for a custom trained model and not an AutoML model.\n",
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=DEPLOYED_NAME,\n",
    "    machine_type=\"c2-standard-8\",\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMH7GrYMlugy"
   },
   "outputs": [],
   "source": [
    "DEPLOYED_NAME = \"penguins_deployed\"\n",
    "\n",
    "# Deploy the model at model endpoint\n",
    "    # The explanation_metadata and explanation_parameters should only be\n",
    "    # provided for a custom trained model and not an AutoML model.\n",
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=deployed_model_display_name,\n",
    "    traffic_percentage=traffic_percentage,\n",
    "    traffic_split=traffic_split,\n",
    "    machine_type=machine_type,\n",
    "    min_replica_count=min_replica_count,\n",
    "    max_replica_count=max_replica_count,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    explanation_metadata=explanation_metadata,\n",
    "    explanation_parameters=explanation_parameters,\n",
    "    metadata=metadata,\n",
    "    sync=sync,\n",
    ")\n",
    "\n",
    "\n",
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=DEPLOYED_NAME,\n",
    "    # Add any other deployment settings you need here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NotFound",
     "evalue": "404 The Model does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:65\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/grpc/_channel.py:1181\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1175\u001b[0m (\n\u001b[1;32m   1176\u001b[0m     state,\n\u001b[1;32m   1177\u001b[0m     call,\n\u001b[1;32m   1178\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1179\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1180\u001b[0m )\n\u001b[0;32m-> 1181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/grpc/_channel.py:1006\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.NOT_FOUND\n\tdetails = \"The Model does not exist.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:172.217.204.95:443 {created_time:\"2024-12-12T13:57:37.987940761+00:00\", grpc_status:5, grpc_message:\"The Model does not exist.\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m path_to_your_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprojects/101459138402/locations/us-east1/models/1462407639546724352\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#model = aiplatform.Model(\"path/to/your/model\")  # TODO\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43maiplatform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath_to_your_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mdeploy(\n\u001b[1;32m      7\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[1;32m      8\u001b[0m     deployed_model_display_name\u001b[38;5;241m=\u001b[39mDEPLOYED_NAME,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Add any other deployment settings you need here\u001b[39;00m\n\u001b[1;32m     10\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/cloud/aiplatform/models.py:4144\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, model_name, project, location, credentials, version)\u001b[0m\n\u001b[1;32m   4142\u001b[0m \u001b[38;5;66;03m# Create a versioned model_name, if it exists, for getting the GCA model\u001b[39;00m\n\u001b[1;32m   4143\u001b[0m versioned_model_name \u001b[38;5;241m=\u001b[39m ModelRegistry\u001b[38;5;241m.\u001b[39m_get_versioned_name(model_name, version)\n\u001b[0;32m-> 4144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_gca_resource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversioned_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4146\u001b[0m \u001b[38;5;66;03m# Create ModelRegistry with the unversioned resource name\u001b[39;00m\n\u001b[1;32m   4147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry \u001b[38;5;241m=\u001b[39m ModelRegistry(\n\u001b[1;32m   4148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresource_name,\n\u001b[1;32m   4149\u001b[0m     location\u001b[38;5;241m=\u001b[39mlocation,\n\u001b[1;32m   4150\u001b[0m     project\u001b[38;5;241m=\u001b[39mproject,\n\u001b[1;32m   4151\u001b[0m     credentials\u001b[38;5;241m=\u001b[39mcredentials,\n\u001b[1;32m   4152\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:692\u001b[0m, in \u001b[0;36mVertexAiResourceNoun._get_gca_resource\u001b[0;34m(self, resource_name, parent_resource_name_fields)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns GAPIC service representation of client class resource.\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;124;03m        Should not include project and location.\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m resource_name \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mfull_resource_name(\n\u001b[1;32m    682\u001b[0m     resource_name\u001b[38;5;241m=\u001b[39mresource_name,\n\u001b[1;32m    683\u001b[0m     resource_noun\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resource_noun,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    689\u001b[0m     resource_id_validator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resource_id_validator,\n\u001b[1;32m    690\u001b[0m )\n\u001b[0;32m--> 692\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getter_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresource_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_DEFAULT_RETRY\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/cloud/aiplatform_v1/services/model_service/client.py:1046\u001b[0m, in \u001b[0;36mModelServiceClient.get_model\u001b[0;34m(self, request, name, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m-> 1046\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:113\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m     metadata\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata)\n\u001b[1;32m    111\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metadata\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/api_core/retry.py:349\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    346\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    348\u001b[0m )\n\u001b[0;32m--> 349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/api_core/retry.py:191\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:67\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mNotFound\u001b[0m: 404 The Model does not exist."
     ]
    }
   ],
   "source": [
    "# Deploy the Model resource to the Endpoint resource\n",
    "path_to_your_model=\"projects/101459138402/locations/us-east1/models/1462407639546724352\"\n",
    "\n",
    "#model = aiplatform.Model(\"path/to/your/model\")  # TODO\n",
    "model = aiplatform.Model(\"path_to_your_model\")  # TODO\n",
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=DEPLOYED_NAME,\n",
    "    # Add any other deployment settings you need here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_prediction"
   },
   "source": [
    "## Task 7. Process the test data and make an online prediction request\n",
    "\n",
    "Send an online prediction request to your deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_test_item:test"
   },
   "source": [
    "Prepare test data by convert it to a Python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "67aeea91384a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_holdout_y = df_holdout.pop(LABEL_COLUMN)\n",
    "df_holdout_x = df_holdout\n",
    "\n",
    "# Convert to list representation\n",
    "holdout_x = np.array(df_holdout_x).tolist()\n",
    "holdout_y = np.array(df_holdout_y).astype(\"float32\").tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "send_prediction_request:image"
   },
   "source": [
    "### Send the prediction request\n",
    "\n",
    "Now that you have test data, you can use it to send a prediction request. Use the `Endpoint` object's `predict` function, which takes the following parameters:\n",
    "\n",
    "- `instances`: A list of penguin measurement instances. According to your custom model, each instance should be an array of numbers. You prepared this list in the previous step.\n",
    "\n",
    "The `predict` function returns a list, where each element in the list corresponds to the an instance in the request. In the output for each prediction, you see the following:\n",
    "\n",
    "- Confidence level for the prediction (`predictions`), between 0 and 1, for each of the ten classes.\n",
    "\n",
    "You can then run a quick evaluation on the prediction results:\n",
    "1. `np.argmax`: Convert each list of confidence levels to a label\n",
    "2. Print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "6e20473b09f5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = endpoint.predict(instances=holdout_x)\n",
    "y_predicted = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_prediction_output(bucket_name, blob_name, predicted_output):\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    with blob.open(\"w\") as f:\n",
    "        f.write(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_prediction_output(f\"{BUCKET_NAME}\", \"prediction.txt\", str(y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "overview:custom",
    "objective:custom,training,online_prediction",
    "dataset:custom,cifar10,icn",
    "costs",
    "7c163842eabd",
    "accelerators:training,prediction",
    "container:training,prediction",
    "machine:training,prediction",
    "59f24e7d2269",
    "5c7732822757",
    "train_custom_model",
    "train_custom_job_args",
    "taskpy_contents",
    "train_custom_job",
    "deploy_model:dedicated",
    "make_prediction",
    "get_test_item:test",
    "send_prediction_request:image",
    "undeploy_model",
    "cleanup:custom"
   ],
   "name": "custom-tabular-bq-managed-dataset.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11 (Local)",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
