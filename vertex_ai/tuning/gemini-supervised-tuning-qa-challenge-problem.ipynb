{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDcI0SYySZiu"
   },
   "source": [
    "# [CEPF L300]: Tune Gemini Model by using Supervised Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D59iF36T62k"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fEBa5FbT-dc"
   },
   "source": [
    "### Install Vertex AI SDK and other required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0M04I5j3_KY5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install --upgrade --user --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyKgURhIUnAM"
   },
   "source": [
    "## Set Project and Location\n",
    "\n",
    "First, you have to set your project_id, location, and bucket_name. You can also use an existing bucket within the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4acO9tVcU1Ey",
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_id_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "PROJECT_ID = project_id_output[0]\n",
    "REGION = !gcloud compute project-info describe --format=\"value[](commonInstanceMetadata.items.google-compute-default-region)\"\n",
    "LOCATION = REGION[0]\n",
    "\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-model-dataset\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkBZ-e85UeiI"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jsnIinC4UfZq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import (\n",
    "    GenerativeModel,\n",
    "    Part,\n",
    "    HarmCategory,\n",
    "    HarmBlockThreshold,\n",
    "    GenerationConfig,\n",
    ")\n",
    "from vertexai.preview.tuning import sft\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "from typing import Union\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbaPSIO4_iur"
   },
   "source": [
    "## Generate the training and validation dataset files\n",
    "\n",
    "To create a tuning job, you use a Q&A with a context dataset in JSON format.\n",
    "\n",
    "Supervised fine-tuning offers a solution, as it allows focused adaptation of foundation models to new tasks. You can create a supervised text model tuning job using the Google Cloud console, API, or the Vertex AI SDK for Python.Â For more information, refer to the [documentation page](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-use-supervised-tuning),\n",
    "\n",
    "But how do you ensure your data is primed for success with supervised fine-tuning? Here are the critical areas to focus on:\n",
    "\n",
    "- **Domain Alignment:** Supervised fine-tuning thrives on smaller datasets, but they must be highly relevant to your downstream task. Look for data that closely mirrors the domain you will encounter in real-world use cases.\n",
    "- **Labeling Accuracy:** Noisy labels sabotage even the best technique. Prioritize accuracy in your annotations and labeling.\n",
    "- **Noise Reduction:** Outliers, inconsistencies, or irrelevant examples hurt model adaptation. Implement preprocessing, such as removing duplicates, fixing typos, and verifying that data conforms to your task's expectations.\n",
    "- **Distribution:** A diverse range of examples helps your model generalize better within the confines of your target task. Refrain from overloading the process with excessive variance that strays from your core domain.\n",
    "- **Balanced Classes:** For classification tasks, try to keep a reasonable balance between different classes to avoid the model learning biases towards a specific class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivFjWO5M-Z8H"
   },
   "source": [
    "### Fetching data from BigQuery\n",
    "\n",
    "Your model tuning dataset must be in a JSONL format where each line contains a single training example. You must make sure that you include instructions.\n",
    "\n",
    "You will use the [StackOverflow dataset](https://cloud.google.com/blog/topics/public-datasets/google-bigquery-public-datasets-now-include-stack-overflow-q-a) on BigQuery Public Datasets, limiting to questions with the `python` tag, and accepted answers for answers since 2020-01-01.\n",
    "\n",
    "You use a helper function to read the data from BigQuery and create a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JIlL-aVbNPg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_bq_query(sql: str) -> Union[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Run a BigQuery query and return the job ID or result as a DataFrame\n",
    "    Args:\n",
    "        sql: SQL query, as a string, to execute in BigQuery\n",
    "    Returns:\n",
    "        df: DataFrame of results from query,  or error, if any\n",
    "    \"\"\"\n",
    "\n",
    "    bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "    # Try dry run before executing query to catch any errors\n",
    "    job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
    "    bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "    # If dry run succeeds without errors, proceed to run query\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    client_result = bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "    job_id = client_result.job_id\n",
    "\n",
    "    # Wait for query/job to finish running. then get & return DataFrame\n",
    "    df = client_result.result().to_arrow().to_pandas()\n",
    "    print(f\"Finished job_id: {job_id}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11WLzqp-b59c"
   },
   "source": [
    "Next you write the query. Limit your example to 550.\n",
    "\n",
    "**TODO:** Update the query below to limit the results to 550."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLC_elwzb3ZF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "stack_overflow_df = run_bq_query(\n",
    "    \"\"\"SELECT\n",
    "           CONCAT(q.title, q.body) AS input_text,\n",
    "           a.body AS output_text\n",
    "       FROM `bigquery-public-data.stackoverflow.posts_questions` q\n",
    "       JOIN `bigquery-public-data.stackoverflow.posts_answers` a\n",
    "         ON q.accepted_answer_id = a.id\n",
    "       WHERE q.accepted_answer_id IS NOT NULL\n",
    "         AND REGEXP_CONTAINS(q.tags, \"python\")\n",
    "         AND a.creation_date >= \"2020-01-01\"\n",
    "       LIMIT TODO\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "stack_overflow_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b404hW8jcRDQ"
   },
   "source": [
    "There should be 550 questions and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUg-lF61cUVI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(stack_overflow_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHda8BzbmRMC"
   },
   "source": [
    "### Adding instructions\n",
    "Finetuning language models on a collection of datasets phrased as instructions improve model performance and generalization to unseen tasks [(Google, 2022)](https://arxiv.org/pdf/2210.11416.pdf).\n",
    "\n",
    "An instruction refers to a specific directive or guideline that conveys a task or action to be executed. These instructions can be expressed in various forms, such as step-by-step procedures, commands, or rules. When you don't use the instructions, it's only a question and answer. The instruction tells the large language model what to do. You want them to answer the question. You have to give a hint about the task you want to perform. Extend the dataset with an instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIy7BjKWmu5j",
    "tags": []
   },
   "outputs": [],
   "source": [
    "INSTRUCTION_TEMPLATE = f\"\"\"\\\n",
    "You are a helpful Python developer \\\n",
    "You are good at answering Stackoverflow questions \\\n",
    "Your mission is to provide developers with helpful answers that work\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tM_f1b3n4TK"
   },
   "source": [
    "Create a new column for the `INSTRUCTION_TEMPLATE`. Use a new column and do not overwrite the existing one because you might want to use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJpAJG8uoE7F",
    "tags": []
   },
   "outputs": [],
   "source": [
    "stack_overflow_df[\"input_text_instruct\"] = INSTRUCTION_TEMPLATE\n",
    "\n",
    "stack_overflow_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNMMxaB2cZvY"
   },
   "source": [
    "**TODO:**\n",
    "Next, you split the data into training and evaluation. For Extractive Q&A tasks, it's advised 500+ training examples. In this case, you use 440 to generate a tuning job that runs faster. \n",
    "\n",
    "20% of your dataset is used for testing. The `random_state` controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdrweRsscfgU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Update the test_size to select 20% of data for evaluation\n",
    "train, evaluation = train_test_split(stack_overflow_df, test_size=TODO, random_state=42)\n",
    "\n",
    "# Warning - Don't change the following print statements. It is used for score tracking. \n",
    "# Please don't forget to save this notebook script.\n",
    "print('Total number of records in training dataset:',len(train))\n",
    "print('Total number of records in validation dataset:',len(evaluation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the **Task 4. Generate the training and validation dataset files** section of the lab instructions and click  **Check my progress** to verify the __Split the dataset for training and evaluation__ objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_MuRRGmfLni"
   },
   "source": [
    "### Generate the JSONL files\n",
    "\n",
    "Prepare your training data in a JSONL (JSON Lines) file and store it in a Google Cloud Storage (GCS) bucket. This format ensures efficient processing. Each line of the JSONL file must represent a single data instance and follow a well-defined schema:\n",
    "\n",
    "`{ \"systemInstruction\": {\"role\": \"system\", \"parts\": [{\"text\": \"instructions\"}]},\n",
    "  \"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"question\"}]},{\"role\": \"model\", \"parts\": [{\"text\": \"answering\"}]}]}`\n",
    "\n",
    "This is how it maps to the Pandas df columns:\n",
    "\n",
    "*   `instructions -> input_text_instruct`\n",
    "*   `question -> input_text`\n",
    "*   `answer -> output_text`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgPXoXOlc0vI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuning_data_filename = f\"tune_data_stack_overflow_qa.jsonl\"\n",
    "validation_data_filename = f\"validation_data_stack_overflow_qa.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-oHmx0wfElN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_messages(row):\n",
    "    \"\"\"Formats a single row into the desired JSONL structure\"\"\"\n",
    "    return {\n",
    "      \"systemInstruction\": {\n",
    "        \"role\": \"system\",\n",
    "        \"parts\": [\n",
    "          {\n",
    "            \"text\": row[\"input_text_instruct\"]\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      \"contents\": [\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"parts\": [\n",
    "            {\n",
    "              \"text\": row[\"input_text\"]\n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"model\",\n",
    "          \"parts\": [\n",
    "            {\n",
    "              \"text\": row[\"output_text\"]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mBwn2jJEkYl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply formatting function to each row, then convert to JSON Lines format\n",
    "tuning_data = train.apply(format_messages, axis=1).to_json(orient=\"records\", lines=True)\n",
    "\n",
    "# Save the result to a JSONL file\n",
    "with open(tuning_data_filename, \"w\") as f:\n",
    "    f.write(tuning_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yz9IbouGftaZ"
   },
   "source": [
    "Next, check if the number of rows match with your Pandas df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4JfgAijikHp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(tuning_data_filename, \"r\") as f:\n",
    "    num_rows = sum(1 for line in f)\n",
    "\n",
    "print(\"Number of rows in the JSONL file:\", num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42u53mHQVZk3"
   },
   "source": [
    "Do the same for the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nBc6ufE0h2zL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply formatting function to each row, then convert to JSON Lines format\n",
    "validation_data = evaluation.apply(format_messages, axis=1).to_json(\n",
    "    orient=\"records\", lines=True\n",
    ")\n",
    "\n",
    "# Save the result to a JSONL file\n",
    "with open(validation_data_filename, \"w\") as f:\n",
    "    f.write(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYGr7h_2ahqb"
   },
   "source": [
    "Next, copy the JSONL files into the Google Cloud Storage bucket you specified or created at the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eq0MYC6nxhKy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil cp $tuning_data_filename $validation_data_filename $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBq0NMIxa2iD"
   },
   "source": [
    "Check if the files are in the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVel0g6pkOiA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsc0xhNGa7ZQ"
   },
   "source": [
    "Create two variables for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXzEZFjtkTWJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "TUNING_DATA_URI = f\"{BUCKET_URI}/{tuning_data_filename}\"\n",
    "VALIDATION_DATA_URI = f\"{BUCKET_URI}/{validation_data_filename}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the **Task 4. Generate the training and validation dataset files** section of the lab instructions and click  **Check my progress** to verify the __Store the training and validation files in Cloud Storage__ objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAOu-xJnA54y"
   },
   "source": [
    "## Start a supervised tuning job using Gemini\n",
    "It's time to start your tuning job. Use the `gemini-1.5-pro-002` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SodJv2vWicfu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "foundation_model = GenerativeModel(\"gemini-1.5-pro-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Create a supervised fine-tuning job with following parameters: \n",
    "\n",
    "* **Tuned model display name:** `StackOverflow Q&A Supervised Tuning Job`\n",
    "* **Source model:** gemini-1.5-pro-002\n",
    "* **Training dataset:** tune_data_stack_overflow_qa.jsonl\n",
    "* **Validation dataset:** validation_data_stack_overflow_qa.jsonl\n",
    "* **Epochs:** 3\n",
    "* **Learning Rate Multiplier:** 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6e7zBH5foZbC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "sft_tuning_job = [ TODO - Insert your code ]\n",
    "\n",
    "# Get the tuning job info.\n",
    "sft_tuning_job.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LSm5Ns5gjx-"
   },
   "source": [
    "Go to the **Task 5. Start a supervised tuning job using Gemini** section of the lab and click  **Check my progress** to verify the __Start a supervised tuning job using Gemini__ objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgukIEFPlVdD"
   },
   "source": [
    "Next, you retrieve the model resource name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3yiKi-KofGK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the resource name of the tuning job\n",
    "sft_tuning_job_name = sft_tuning_job.resource_name\n",
    "sft_tuning_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SM1RZVqWKRdg"
   },
   "source": [
    "Tuning takes approximately 100-120 minutes. Wait until the job is finished before you continue after the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uyug1dw4FAgn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Wait for job completion\n",
    "while not sft_tuning_job.refresh().has_ended:\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the tuning job, go to the **Task 5. Start a supervised tuning job using Gemini** section of the lab and click  **Check my progress** to verify the __Tune Gemini model using supervised fine-tuning__ objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyDS9G2TTX9p"
   },
   "outputs": [],
   "source": [
    "# tuned model name\n",
    "tuned_model_name = sft_tuning_job.tuned_model_name\n",
    "tuned_model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_s57xpI5o9m0"
   },
   "source": [
    "Use `tuning.TuningJob.list()` to retrieve your tuning jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8QtT3uJ3Jw0N"
   },
   "outputs": [],
   "source": [
    "sft_tuning_job.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KQmyjjcJ9uz"
   },
   "source": [
    "Your model is automatically deployed as a Vertex AI Endpoint and ready for use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X9uQD-Ee_h6h"
   },
   "outputs": [],
   "source": [
    "# tuned model endpoint name\n",
    "tuned_model_endpoint_name = sft_tuning_job.tuned_model_endpoint_name\n",
    "tuned_model_endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRPlb4ZO8ulD"
   },
   "source": [
    "## Test the tuned model with a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhfU4wTOtH1y"
   },
   "outputs": [],
   "source": [
    "tuned_model = GenerativeModel(tuned_model_endpoint_name)\n",
    "print(tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1q1PT2zJRO9"
   },
   "outputs": [],
   "source": [
    "question = \"How do I store a TensorFlow checkpoint on Google Cloud Storage while training?\"\n",
    "response = tuned_model.generate_content(question)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the response from model in the Cloud Storage bucket \n",
    "from google.cloud import storage\n",
    "\n",
    "with open('tuned_model_qa_response.txt', \"w+\") as output:\n",
    "    image_content = output.write(response.text)\n",
    "    output.close()\n",
    "\n",
    "storage_client = storage.Client()\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-model-dataset\"\n",
    "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "blob = bucket.blob('tuned_model_qa_response.txt')\n",
    "blob.upload_from_filename('tuned_model_qa_response.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the **Task 6. Test the tuned model with a prompt** section of the lab and click  **Check my progress** to verify the __Test the tuned model with a prompt__ objective."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
